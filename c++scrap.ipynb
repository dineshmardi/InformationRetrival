{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa651a5-0cdd-412a-b66f-bb6e343e083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed with 20 items.\n",
      "Page 2 processed with 20 items.\n",
      "Page 3 processed with 20 items.\n",
      "Page 4 processed with 20 items.\n",
      "Page 5 processed with 20 items.\n",
      "Page 6 processed with 20 items.\n",
      "Page 7 processed with 20 items.\n",
      "Page 8 processed with 20 items.\n",
      "Page 9 processed with 20 items.\n",
      "Page 10 processed with 20 items.\n",
      "Page 11 processed with 10 items.\n",
      "Page 12 does not exist. Ending scraping.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 5 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 6 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 7 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 8 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 9 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 10 in subsection https://cplusplus.com/articles/ processed with 20 items.\n",
      "Page 11 in subsection https://cplusplus.com/articles/ processed with 10 items.\n",
      "Page 12 in subsection https://cplusplus.com/articles/ does not exist. Moving to next subsection.\n",
      "Page 1 in subsection https://cplusplus.com/articles/algorithms/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/algorithms/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/cpp11/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/cpp11/ processed with 6 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/graphics/ processed with 12 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/howto/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/howto/ processed with 10 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/language/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/language/ processed with 9 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/linux/ processed with 11 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/sourcecode/ processed with 20 items.\n",
      "Page 4 in subsection https://cplusplus.com/articles/sourcecode/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/standard_library/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/standard_library/ processed with 3 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tips/ processed with 20 items.\n",
      "Page 3 in subsection https://cplusplus.com/articles/tips/ processed with 15 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/tools/ processed with 20 items.\n",
      "Page 2 in subsection https://cplusplus.com/articles/tools/ processed with 5 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/visualcpp/ processed with 20 items.\n",
      "Page 1 in subsection https://cplusplus.com/articles/winapi/ processed with 20 items.\n",
      "Data has been written to cplusplus_articles.csv. Total articles retrieved: 9351\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "base_url = \"https://cplusplus.com/articles/?page=\"\n",
    "base_domain = \"https://cplusplus.com\"\n",
    "\n",
    "items = []\n",
    "page_no = 1\n",
    "\n",
    "# Step 1: Scrape main articles page to find subsection links (like Algorithms)\n",
    "subsection_links = []\n",
    "\n",
    "while True:\n",
    "    response = requests.get(base_url + str(page_no))\n",
    "\n",
    "    if response.status_code == 404:\n",
    "        print(f\"Page {page_no} does not exist. Ending scraping.\")\n",
    "        break\n",
    "    elif response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page no {page_no}: {response.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all subsection links (e.g., \"Algorithms\")\n",
    "    subsections = soup.find_all('div', class_='sect')  # Adjust class if necessary\n",
    "    for subsection in subsections:\n",
    "        links = subsection.find_all('a')\n",
    "        for link in links:\n",
    "            subsection_links.append(base_domain + link['href'])\n",
    "\n",
    "    # Process articles on this page\n",
    "    s_items = soup.find_all('td', class_='elem')\n",
    "\n",
    "    if not s_items:\n",
    "        break\n",
    "\n",
    "    for s_item in s_items:\n",
    "        anchor = s_item.find('a')\n",
    "        \n",
    "        if anchor:\n",
    "            title = anchor.find('span', class_='title').text.strip()\n",
    "            link = anchor['href'].strip()\n",
    "            if link.startswith('/'):\n",
    "                link = base_domain + link\n",
    "            items.append({'TITLE': title, 'URL': link, 'SECTION': 'articles'})\n",
    "\n",
    "    print(f\"Page {page_no} processed with {len(s_items)} items.\")\n",
    "    page_no += 1\n",
    "\n",
    "# Step 2: Scrape each subsection link\n",
    "for link in subsection_links:\n",
    "    page_no = 1\n",
    "    while True:\n",
    "        response = requests.get(link + f\"?page={page_no}\")\n",
    "        \n",
    "        if response.status_code == 404:\n",
    "            print(f\"Page {page_no} in subsection {link} does not exist. Moving to next subsection.\")\n",
    "            break\n",
    "        elif response.status_code != 200:\n",
    "            raise Exception(f\"Failed to load page no {page_no} in subsection {link}: {response.status_code}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        s_items = soup.find_all('td', class_='elem')\n",
    "\n",
    "        if not s_items:\n",
    "            break\n",
    "\n",
    "        for s_item in s_items:\n",
    "            anchor = s_item.find('a')\n",
    "            \n",
    "            if anchor:\n",
    "                title = anchor.find('span', class_='title').text.strip()\n",
    "                article_link = anchor['href'].strip()\n",
    "                if article_link.startswith('/'):\n",
    "                    article_link = base_domain + article_link\n",
    "                items.append({'TITLE': title, 'URL': article_link, 'SECTION': 'subsection'})\n",
    "\n",
    "        print(f\"Page {page_no} in subsection {link} processed with {len(s_items)} items.\")\n",
    "        page_no += 1\n",
    "\n",
    "# Write all collected items to CSV\n",
    "csv_name = 'cplusplus_articles.csv'\n",
    "with open(csv_name, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['TITLE', 'URL', 'SECTION']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for ite in items:\n",
    "        writer.writerow({'TITLE': ite['TITLE'], 'URL': ite['URL'], 'SECTION': ite['SECTION']})\n",
    "\n",
    "print(f\"Data has been written to {csv_name}. Total articles retrieved: {len(items)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46f925f-1f2e-4d9c-bbeb-3db1f1acbdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Tokenized data saved to tokenized_cplusplus_articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer for word tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 1: Load the CSV file into a DataFrame\n",
    "input_csv = 'cplusplus_articles.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Tokenize the text in a specific column\n",
    "# Assuming you want to tokenize the 'TITLE' column\n",
    "df['TOKENIZED_TITLE'] = df['TITLE'].apply(word_tokenize)\n",
    "\n",
    "# Step 3: Save the tokenized data to a new CSV file\n",
    "output_csv = 'tokenized_cplusplus_articles.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Tokenization complete. Tokenized data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b057e09-0587-429b-b0ca-c7be70af274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalization complete. Normalized data saved to normalized_cplusplus_articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "# Download the Punkt tokenizer for word tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Join tokens back to string (if you want to keep it tokenized, you can skip this step)\n",
    "    normalized_text = ' '.join(tokens)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Step 1: Load the CSV file into a DataFrame\n",
    "input_csv = 'cplusplus_articles.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Apply normalization to the 'TITLE' column\n",
    "df['NORMALIZED_TITLE'] = df['TITLE'].apply(normalize_text)\n",
    "\n",
    "# Step 3: Save the normalized data to a new CSV file\n",
    "output_csv = 'normalized_cplusplus_articles.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Text normalization complete. Normalized data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca83c228-b2bf-475e-b5af-5d22665315fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removal complete. Data saved to no_stopwords_cplusplus_articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Join tokens back to string (if you want to keep it tokenized, you can skip this step)\n",
    "    normalized_text = ' '.join(tokens)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Use a set for faster lookup\n",
    "    tokens = word_tokenize(text)  # Tokenize the text again if needed\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Filter out stop words\n",
    "    filtered_text = ' '.join(filtered_tokens)  # Join filtered tokens back into a string\n",
    "    return filtered_text\n",
    "\n",
    "# Step 1: Load the CSV file into a DataFrame\n",
    "input_csv = 'normalized_cplusplus_articles.csv'  # Your normalized CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Remove stop words from the 'NORMALIZED_TITLE' column\n",
    "df['TITLE_NO_STOPWORDS'] = df['NORMALIZED_TITLE'].apply(remove_stop_words)\n",
    "\n",
    "# Step 3: Save the data with stop words removed to a new CSV file\n",
    "output_csv = 'no_stopwords_cplusplus_articles.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Stop words removal complete. Data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0788ee5e-6e87-41af-9724-cd9ad5f012fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing complete. Data saved to new_processed_cplusplus_articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Join tokens back to string (if you want to keep it tokenized, you can skip this step)\n",
    "    normalized_text = ' '.join(tokens)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Use a set for faster lookup\n",
    "    tokens = word_tokenize(text)  # Tokenize the text again if needed\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Filter out stop words\n",
    "    filtered_text = ' '.join(filtered_tokens)  # Join filtered tokens back into a string\n",
    "    return filtered_text\n",
    "\n",
    "# Function for stemming using Porter Stemmer algorithm\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "\n",
    "# Function for lemmatization using WordNet\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Step 1: Load the CSV file into a DataFrame\n",
    "input_csv = 'cplusplus_articles.csv'  # Use the file after normalization and stop word removal\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Apply stop word removal to the 'NORMALIZED_TITLE' column\n",
    "df['TITLE_NO_STOPWORDS'] = df['TITLE'].apply(remove_stop_words)\n",
    "\n",
    "# Step 3: Apply stemming and lemmatization to the 'TITLE_NO_STOPWORDS' column\n",
    "df['STEMMED_TITLE'] = df['TITLE_NO_STOPWORDS'].apply(stem_text)\n",
    "df['LEMMATIZED_TITLE'] = df['TITLE_NO_STOPWORDS'].apply(lemmatize_text)\n",
    "\n",
    "# Step 4: Save the processed data to a new CSV file\n",
    "output_csv = 'new_processed_cplusplus_articles.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Text processing complete. Data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69611149-7a65-4454-aab8-d5042123e694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'running']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "a=\"i am running\"\n",
    "t=word_tokenize(a)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad4ac7f-29ab-4a5e-93ae-8bd2729518e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing complete. Data saved to update_processed_cplusplus_articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Join tokens back to string (if you want to keep it tokenized, you can skip this step)\n",
    "    normalized_text = ' '.join(tokens)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Use a set for faster lookup\n",
    "    tokens = word_tokenize(text)  # Tokenize the text again if needed\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Filter out stop words\n",
    "    filtered_text = ' '.join(filtered_tokens)  # Join filtered tokens back into a string\n",
    "    return filtered_text\n",
    "\n",
    "# Function for stemming using Porter Stemmer algorithm\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "\n",
    "# Function for lemmatization using WordNet\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Step 1: Load the CSV file into a DataFrame\n",
    "input_csv = 'cplusplus_articles.csv'  # Your initial CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Apply normalization to the 'TITLE' column\n",
    "df['NORMALIZED_TITLE'] = df['TITLE'].apply(normalize_text)\n",
    "\n",
    "# Step 3: Remove stop words from the 'NORMALIZED_TITLE' column\n",
    "df['TITLE_NO_STOPWORDS'] = df['NORMALIZED_TITLE'].apply(remove_stop_words)\n",
    "\n",
    "# Step 4: Apply stemming and lemmatization to the 'TITLE_NO_STOPWORDS' column\n",
    "df['STEMMED_TITLE'] = df['TITLE_NO_STOPWORDS'].apply(stem_text)\n",
    "df['LEMMATIZED_TITLE'] = df['TITLE_NO_STOPWORDS'].apply(lemmatize_text)\n",
    "\n",
    "# Step 5: Save the processed data to a new CSV file\n",
    "output_csv = 'update_processed_cplusplus_articles.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Text processing complete. Data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9b3c4a-7f4d-4c02-82ad-bf9739d3ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing complete. Data saved to pro_processed_cplusplus_articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Define a translation table that removes punctuation except '+'\n",
    "    translator = str.maketrans('', '', string.punctuation.replace('+', ''))\n",
    "    # Remove punctuation except '+'\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Join tokens back to string (if you want to keep it tokenized, you can skip this step)\n",
    "    normalized_text = ' '.join(tokens)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Use a set for faster lookup\n",
    "    tokens = word_tokenize(text)  # Tokenize the text again if needed\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Filter out stop words\n",
    "    filtered_text = ' '.join(filtered_tokens)  # Join filtered tokens back into a string\n",
    "    return filtered_text\n",
    "\n",
    "# Function for stemming using Porter Stemmer algorithm\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "\n",
    "# Function for lemmatization using WordNet\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Step 1: Load the CSV file into a DataFrame\n",
    "input_csv = 'cplusplus_articles.csv'  # Your initial CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Apply normalization to the 'TITLE' column\n",
    "df['NORMALIZED_TITLE'] = df['TITLE'].apply(normalize_text)\n",
    "\n",
    "# Step 3: Remove stop words from the 'NORMALIZED_TITLE' column\n",
    "df['TITLE_NO_STOPWORDS'] = df['NORMALIZED_TITLE'].apply(remove_stop_words)\n",
    "\n",
    "# Step 4: Apply stemming and lemmatization to the 'TITLE_NO_STOPWORDS' column\n",
    "df['STEMMED_TITLE'] = df['TITLE_NO_STOPWORDS'].apply(stem_text)\n",
    "df['LEMMATIZED_TITLE'] = df['TITLE_NO_STOPWORDS'].apply(lemmatize_text)\n",
    "\n",
    "# Step 5: Save the processed data to a new CSV file\n",
    "output_csv = 'pro_processed_cplusplus_articles.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Text processing complete. Data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11cc0d1-1d84-4f1b-b666-525d5198a369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology Created:\n",
      "\n",
      "Word: c++\n",
      "  Synonyms: []\n",
      "  Antonyms: []\n",
      "  Hypernyms: []\n",
      "\n",
      "Word: interpreter\n",
      "  Synonyms: ['interpretive_program', 'spokesperson', 'translator', 'representative', 'interpreter', 'voice']\n",
      "  Antonyms: []\n",
      "  Hypernyms: ['program', 'advocate', 'person', 'mediator']\n",
      "\n",
      "Word: token\n",
      "  Synonyms: ['tokenish', 'relic', 'souvenir', 'token', 'keepsake', 'nominal', 'item']\n",
      "  Antonyms: []\n",
      "  Hypernyms: ['sign', 'disk', 'object', 'symbol']\n",
      "\n",
      "Word: program\n",
      "  Synonyms: ['curriculum', 'political_program', 'plan', 'platform', 'political_platform', 'computer_programme', 'syllabus', 'course_of_study', 'computer_program', 'program', 'broadcast', 'programme']\n",
      "  Antonyms: []\n",
      "  Hypernyms: ['system', 'software', 'announcement', 'information', 'schedule', 'document', 'show', 'create_by_mental_act', 'performance', 'idea']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dines\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def get_antonyms(word):\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.antonyms()[0].name())\n",
    "    return antonyms\n",
    "\n",
    "def get_hypernyms(word):\n",
    "    hypernyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for hypernym in syn.hypernyms():\n",
    "            hypernyms.add(hypernym.name().split('.')[0])  # Get the name of the hypernym\n",
    "    return hypernyms\n",
    "\n",
    "# Example words\n",
    "words = ['c++', 'interpreter', 'token','program']\n",
    "\n",
    "# Create ontology\n",
    "ontology = {}\n",
    "for word in words:\n",
    "    ontology[word] = {\n",
    "        'synonyms': list(get_synonyms(word)),\n",
    "        'antonyms': list(get_antonyms(word)),\n",
    "        'hypernyms': list(get_hypernyms(word))\n",
    "    }\n",
    "\n",
    "print(\"Ontology Created:\")\n",
    "for word, relationships in ontology.items():\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"  Synonyms: {relationships['synonyms']}\")\n",
    "    print(f\"  Antonyms: {relationships['antonyms']}\")\n",
    "    print(f\"  Hypernyms: {relationships['hypernyms']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1254e1d9-93c7-40c7-909a-8cdae9fa97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9351"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['TITLE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1d2da2-eb3a-45bb-8758-b88598c5d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'to', 'avoid', 'bugs', 'using', 'modern', 'c++', 'learning', 'computer', 'programming', 'terminology', 'class', 'for', 'generate', 'fibonacci', 'series', 'casting', 'safe', 'clearing', 'of', 'private', 'data', 'i', 'learned', 'a', 'vital', 'borland', 'coding', 'technique', \"couldn't\", 'learn', 'alone', 'sierpinski', 'triangle', 'fractal', '-', 'the', 'easiest', 'way', 'produce', 'randomness', 'koch', 'one', 'algorithms', 'with', 'graphics', '10', 'tips', 'be', 'productive', 'in', 'clion,', 'cross-platform', 'c/c++', 'ide', 'fibonacii', 'at', 'its', 'best', 'null', 'pointer', 'dereferencing', 'causes', 'undefined', 'behavior', 'android', 'kernel:', 'lacking', 'modularity', 'design', 'pattern', 'state,', 'simple', 'problem', 'semaphore', 'opengl', 'animation', 'glfw,', 'step', 'by', 'split', 'string', 'declarations,', 'prototypes,', 'definitions,', 'and', 'implementations', 'finding', \"skype's\", 'default', 'account', 'name', 'virtual', 'method', 'table', 'accident', 'prevention', 'dynamically', 'sorting', 'both', 'keys', 'values', 'installing', 'configuring', 'visual', 'glut', 'send', 'an', 'sms', 'message', 'from', 'application', 'strings', 'obfuscation', 'system', 'google', 'docs', 'based', 'backup', 'topological', 'sort', 'strings-', 'part', 'algorithm', 'stdafx.h', 'novices', 'auxiliarry', 'carry', '&', 'parity', 'detector', 'element', 'project', '(v2)', 'last', 'line', 'effect', 'isitqt', 'check', 'if', 'program', 'is', 'qt-based', 'which', 'version', 'qt', 'was', 'used', 'what', 'inline', 'functions', 'open', 'source', 'lightweight', 'cross', 'platform', 'log', 'library:', 'log4z', 'powerball', 'lottery', 'simulator', 'revamped', 'turbo', '3.1', 'code', 'example', 'list', 'box', 'making', 'plugin', '5.02', 'print', 'preview', 'window', 'use', 'windows', 'authorization', 'memcache++:', 'difference', 'between', 'compiler', 'std::vectors,', 'not', 'dynamic', 'arrays!', 'effective', 'keylogger', 'linked', 'lists', 'compiling', 'linking', 'helpful', 'function', 'registry', 'recursive', 'hanoy', 'tower', '(by', 'disch)', \"don't\", 'write', 'any', 'variable', 'larger', 'than', '1', 'byte', 'binary', 'files', 'console', 'game!', 'ternary', 'search.', 'develop', 'operating', 'c++?', 'big', 'calculator', 'gone', 'crazy', 'bmp', 'loader', 'grounded', 'pointers', 'snake', 'cpp', '[console]', \"xistenial's\", 'tuts', 'classes', 'objects', 'lesson', '(v1)', 'tic-tac-toe', 'wade', 'unknown', 'waters.', 'four.', 'can', 'command', 'my', 'application?', '[wip]', 'genetic', 'recursion', 'tchar', '\"stdafx.h\"', 'beginners', 'guide', 'std::sort()', 'port', 'scanner', 'v1.2.0', 'stable', 'publish', 'features', 'c99', 'tags', 'add', 'color', 'your', '2', 'sqlite', 'wrapper', 'lzw', 'file', 'compressor', 'subnet', 'logger', 'v1.0.0', 'non-recursive', 'traversal', 'n-trees', 'v0.9.0', 'brainf*ck-to-c', 'translator', 'customizable', 'asteroids', 'game.', 'make', 'caesar', 'cipher', 'animated', 'checkers', 'game', 'sfml', 'card', '(rummy),', 'user', 'vs.', 'pc', 'patterns', 'real', 'projects:', 'rigsofrods', 'case', 'study.', 'format', 'integers', 'commas', 'separator', 'only', 'chars', '3', 'dfferent', 'ways', 'references', 'split()', 'debugging', 'gdb:', 'find', 'segfault', 'text', 'into', 'two', 'or', 'more', 'characters', \"disch's\", 'tutorial', 'good', 'convert', 'numerical', 'amount', 'verbal', 'integer', 'overflow', 'hmenu', 'priority_queue', 'c++11', 'new', 'variadic', 'templates', 'quantum', 'computing', 'static', 'analysis', 'pointerssimplified', 'cplusplus.com', 'codes', 'input', 'correction', 'password', 'creator', '(in', 'c)', 'hex', 'viewer', 'winapi', 'win32', 'multi-function', 'stopwatch', 'rot13', 'cypher', 'error', 'logging', 'xor', 'encryption', 'decimal', 'radix', 'conversion', 'random', 'number', 'generator', 'save', 'time', 'writing', 'microsoft', 'studio', '11?', 'paranthesis', 'checker', 'maximum', 'three', 'numbers?', '(for', 'beginners)', 'handle', 'exception', 'constructor', \"boost's\", 'shared', 'ptr', 'craft', 'sample', 'c', 'basics', '2/5', '1/5', 'improvements', 'over', 'c++03', 'template', 'parameter', 'deduction', 'array', 'dimensions', 'explicit', 'instantiation', 'decorating', 'alternative', 'construction', 'methods', 'secrets', 'boost', 'revealed:', 'checked', 'delete', 'c++0x', 'suffix', 'return', 'types', '2008', '/', '2010', \"gotcha's\", 'map<', 'string,', 'obj', '>', 'vs', 'obj*', 'rounding', 'discussion', 'search', 'tree', 'searching', 'why', 'you', 'should', 'posts', 'after', 'getting', 'answer.', '\"how', 'ask', 'questions\":', 'itemized', 'thread/rehash', 'moving', 'along', 'lines', \"we've\", 'deprecated', 'bloodshed', 'dev-c++.', 'powerful', 'windows/vc++', '(color,', 'cursor,', '...)', 'choose', 'correct', 'section', 'post', 'we', 'are', 'bunch', 'tough', 'lovers.', 'when', 'language', 'terrible', 'medium', 'games', 'cleaner', 'allocate', 'structure', 'possible', 'lowercase', 'alpha', 'distinguish', 'comparison', 'popular', 'compilers', 'ides', 'qxorm', ':', 'persistence', '(orm),', 'serialization,', 'reflection', 'bison', 'tracking', 'printer', 'stream', 'stdiostream', 'top', 'porting', 'map', 'console.', 'improving', 'type', 'safety', 'security', 'sequence', 'containers', 'advantages,', 'disadvantages,', 'performance', 'guidelines', '?', 'pass', 'parameters', 'value,', 'reference,', 'alternate', 'getch()', 'book', 'brilliant', 'things', 'clone', 'erasure', 'copy', 'constructors,', 'assignment', 'operators,', 'member', 'initialization', 'act', 'multidimentional', 'arrays', 'evil', 'winapi:', 'being', 'unicode', 'friendly', 'masking', 'dll', 'symbols', 'pig', 'latin', 'xd', 'converting', 'numbers', 'to:', 'questions', 'smart', 'conditional', '(or', 'ternary)', 'operator', '(?:)', 'lapack', 'parse', 'parameters.', 'beginner', 'exercises', 'named', 'pipes', 'exchange', 'vista', 'interaction', 'services', 'applications', 'level', 'system()', 'prefer', 'std', 'solutions', 'hand', 'written', 'copycats', 'headers', 'includes:', 'clear', 'screen', 'get', 'ip', 'address', 'target', 'hostname', '(part', '1)', 'topic', 'multi-dimensional', 'keep', 'long', 'enough', 'see', \"program's\", 'output', 'compilers,', 'ides,', 'debuggers', 'jazz', 'bit', 'bitwise', 'cin', 'input.', 'strtok', 'uses', 'double', 'it', 'conditionals', 'true', 'false', 'story', 'recusion', 'allocation', 'tired', 'shifting', 'bits?', 'want', 'build', 'but', 'where', 'start?']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_csv='pro_processed_cplusplus_articles.csv'\n",
    "df=pd.read_csv(input_csv)\n",
    "tokens=[]\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    words=row['TITLE'].lower().split()\n",
    "    for word in words:\n",
    "        if word not in tokens:\n",
    "            tokens.append(word)\n",
    "    \n",
    "\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f0b368-270a-4183-95b3-cdb45587a175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6b73d84-f5a5-45e1-adae-98d64a51b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "input_csv = 'pro_processed_cplusplus_articles.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Assume the text data is in a column named 'corpus'\n",
    "corpus = df['TITLE'].tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c15eee-0752-4f67-b6d9-7bd52c243899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dines\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dines\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cd571fc-483c-4993-ad82-f05b51b08683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token frequencies created and saved to token_frequencies.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load the processed data\n",
    "input_csv = 'pro_processed_cplusplus_articles.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Extract tokens from the 'TITLE_NO_STOPWORDS' column\n",
    "tokens = []\n",
    "for index, row in df.iterrows():\n",
    "    words = row['TITLE_NO_STOPWORDS'].lower().split()\n",
    "    for word in words:\n",
    "        if word not in tokens:\n",
    "            tokens.append(word)\n",
    "\n",
    "# Calculate token frequencies\n",
    "token_frequencies = Counter()\n",
    "\n",
    "# Iterate over each title to count token frequencies\n",
    "for index, row in df.iterrows():\n",
    "    title_words = row['TITLE_NO_STOPWORDS'].lower().split()\n",
    "    frequencies = Counter(title_words)\n",
    "    token_frequencies.update(frequencies)\n",
    "\n",
    "# Convert token frequencies to a DataFrame\n",
    "token_freq_df = pd.DataFrame(token_frequencies.items(), columns=['Token', 'Frequency'])\n",
    "\n",
    "# Save Token Frequencies to CSV\n",
    "token_freq_csv = 'token_frequencies.csv'\n",
    "token_freq_df.to_csv(token_freq_csv, index=False)\n",
    "\n",
    "print(f\"Token frequencies created and saved to {token_freq_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4c83006-4aa1-4688-a681-51ce0917133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-Document Matrix created and saved to term_document_matrix.csv.\n",
      "Inverted Index created and saved to inverted_index.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed data\n",
    "input_csv = 'pro_processed_cplusplus_articles.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Extract unique tokens from the 'TITLE_NO_STOPWORDS' column\n",
    "tokens = []\n",
    "for index, row in df.iterrows():\n",
    "    words = row['TITLE_NO_STOPWORDS'].lower().split()\n",
    "    for word in words:\n",
    "        if word not in tokens:\n",
    "            tokens.append(word)\n",
    "\n",
    "# Create Term-Document Matrix\n",
    "term_document_matrix = []\n",
    "for index, row in df.iterrows():\n",
    "    title_words = row['TITLE_NO_STOPWORDS'].lower().split()\n",
    "    vector = [1 if token in title_words else 0 for token in tokens]\n",
    "    term_document_matrix.append(vector)\n",
    "\n",
    "# Convert Term-Document Matrix to DataFrame\n",
    "tdm_df = pd.DataFrame(term_document_matrix, columns=tokens, index=[i + 1 for i in range(len(df))])\n",
    "\n",
    "# Save Term-Document Matrix to CSV\n",
    "tdm_csv = 'term_document_matrix.csv'\n",
    "tdm_df.to_csv(tdm_csv)\n",
    "\n",
    "print(f\"Term-Document Matrix created and saved to {tdm_csv}.\")\n",
    "\n",
    "# Create Inverted Index\n",
    "inverted_index = {token: [] for token in tokens}\n",
    "for index, row in df.iterrows():\n",
    "    title_id = index + 1  # Adjust for 1-based indexing\n",
    "    title_words = row['TITLE_NO_STOPWORDS'].lower().split()\n",
    "    for token in tokens:\n",
    "        if token in title_words:\n",
    "            if title_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(title_id)\n",
    "\n",
    "# Save Inverted Index to CSV\n",
    "inverted_index_csv = 'inverted_index.csv'\n",
    "with open(inverted_index_csv, 'w') as f:\n",
    "    for token, doc_ids in inverted_index.items():\n",
    "        f.write(f\"{token},{','.join(map(str, doc_ids))}\\n\")\n",
    "\n",
    "print(f\"Inverted Index created and saved to {inverted_index_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd6a47a2-38fa-4f9a-a7db-e51e107753e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency and Posting List saved to term_frequency_posting_list.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the processed data\n",
    "input_csv = 'pro_processed_cplusplus_articles.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Initialize the term frequency and posting list dictionary\n",
    "term_frequency = defaultdict(int)\n",
    "posting_list = defaultdict(list)\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Convert the sentence index from DataFrame to a 1-based index\n",
    "    doc_id = index + 1\n",
    "    \n",
    "    # Get the tokens for the current title (assuming 'TITLE_NO_STOPWORDS' column)\n",
    "    title_words = row['TITLE_NO_STOPWORDS'].lower().split()\n",
    "    \n",
    "    # Create a set of unique words to avoid counting the same word multiple times in a single document\n",
    "    unique_words = set(title_words)\n",
    "    \n",
    "    # Update term frequency and posting list\n",
    "    for word in unique_words:\n",
    "        term_frequency[word] += 1\n",
    "        posting_list[word].append(doc_id)\n",
    "\n",
    "# Prepare the DataFrame to display term frequency and posting list\n",
    "data = {\n",
    "    'words': [],\n",
    "    'term_frequency': [],\n",
    "    'posting_list': []\n",
    "}\n",
    "\n",
    "for word in sorted(term_frequency):\n",
    "    data['words'].append(word)\n",
    "    data['term_frequency'].append(term_frequency[word])\n",
    "    data['posting_list'].append(' -->'.join(map(str, posting_list[word])) + ' -->')\n",
    "\n",
    "# Create the DataFrame\n",
    "result_df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV if needed\n",
    "result_csv = 'term_frequency_posting_list.csv'\n",
    "result_df.to_csv(result_csv, index=False)\n",
    "print(f\"Term Frequency and Posting List saved to {result_csv}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1aeecfc1-233e-42e4-810c-e276a7ba3ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the search string (tokens separated by space):  binary search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 'binary' is found in documents: [47, 90, 133, 257, 300, 343, 467, 510, 553, 655, 742, 941, 1088, 1131, 1174, 1298, 1341, 1384, 1486, 1573, 1772, 1919, 1962, 2005, 2129, 2172, 2215, 2317, 2404, 2603, 2750, 2793, 2836, 2960, 3003, 3046, 3148, 3235, 3434, 3581, 3624, 3667, 3791, 3834, 3877, 3979, 4066, 4265, 4412, 4455, 4498, 4622, 4665, 4708, 4810, 4897, 5096, 5243, 5286, 5329, 5453, 5496, 5539, 5641, 5728, 5927, 6074, 6117, 6160, 6284, 6327, 6370, 6472, 6559, 6758, 6905, 6948, 6991, 7115, 7158, 7201, 7303, 7390, 7589, 7736, 7779, 7822, 7946, 7989, 8032, 8134, 8221, 8420, 8567, 8610, 8653, 8777, 8820, 8863, 8965, 9052, 9251]\n",
      "Token 'search' is found in documents: [49, 133, 259, 343, 469, 553, 655, 855, 1090, 1174, 1300, 1384, 1486, 1686, 1921, 2005, 2131, 2215, 2317, 2517, 2752, 2836, 2962, 3046, 3148, 3348, 3583, 3667, 3793, 3877, 3979, 4179, 4414, 4498, 4624, 4708, 4810, 5010, 5245, 5329, 5455, 5539, 5641, 5841, 6076, 6160, 6286, 6370, 6472, 6672, 6907, 6991, 7117, 7201, 7303, 7503, 7738, 7822, 7948, 8032, 8134, 8334, 8569, 8653, 8779, 8863, 8965, 9165]\n"
     ]
    }
   ],
   "source": [
    "input_search = input(\"Enter the search string (tokens separated by space): \")\n",
    "tokens_to_search = input_search.lower().split()\n",
    "\n",
    "# Search tokens without a function\n",
    "search_results = {}\n",
    "\n",
    "# Directly search for each token in the posting list\n",
    "for token in tokens_to_search:\n",
    "    # Fetch the posting list for the token\n",
    "    if token in posting_list:\n",
    "        search_results[token] = posting_list[token]\n",
    "    else:\n",
    "        search_results[token] = []\n",
    "\n",
    "# Display the search results\n",
    "for token, docs in search_results.items():\n",
    "    if docs:\n",
    "        print(f\"Token '{token}' is found in documents: {docs}\")\n",
    "    else:\n",
    "        print(f\"Token '{token}' is not found in any document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93ecf7ff-00cc-4328-af2d-6baf5c9b6d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the search sentence:  dinesh is trying to learn c++ and java and mastered in pointers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 'dinesh' is not found in any document.\n",
      "Token 'is' is not found in any document.\n",
      "Token 'trying' is not found in any document.\n",
      "Token 'to' is not found in any document.\n",
      "Token 'learn' is found in documents: [6, 83, 120, 121, 216, 293, 330, 331, 426, 503, 540, 541, 708, 757, 758, 836, 925, 991, 1015, 1047, 1124, 1161, 1162, 1257, 1334, 1371, 1372, 1539, 1588, 1589, 1667, 1756, 1822, 1846, 1878, 1955, 1992, 1993, 2088, 2165, 2202, 2203, 2370, 2419, 2420, 2498, 2587, 2653, 2677, 2709, 2786, 2823, 2824, 2919, 2996, 3033, 3034, 3201, 3250, 3251, 3329, 3418, 3484, 3508, 3540, 3617, 3654, 3655, 3750, 3827, 3864, 3865, 4032, 4081, 4082, 4160, 4249, 4315, 4339, 4371, 4448, 4485, 4486, 4581, 4658, 4695, 4696, 4863, 4912, 4913, 4991, 5080, 5146, 5170, 5202, 5279, 5316, 5317, 5412, 5489, 5526, 5527, 5694, 5743, 5744, 5822, 5911, 5977, 6001, 6033, 6110, 6147, 6148, 6243, 6320, 6357, 6358, 6525, 6574, 6575, 6653, 6742, 6808, 6832, 6864, 6941, 6978, 6979, 7074, 7151, 7188, 7189, 7356, 7405, 7406, 7484, 7573, 7639, 7663, 7695, 7772, 7809, 7810, 7905, 7982, 8019, 8020, 8187, 8236, 8237, 8315, 8404, 8470, 8494, 8526, 8603, 8640, 8641, 8736, 8813, 8850, 8851, 9018, 9067, 9068, 9146, 9235, 9301, 9325]\n",
      "Token 'c++' is found in documents: [1, 3, 4, 6, 20, 25, 31, 32, 34, 36, 38, 43, 48, 50, 57, 58, 62, 70, 91, 113, 116, 120, 121, 151, 166, 179, 191, 210, 211, 213, 214, 216, 230, 235, 241, 242, 244, 246, 248, 253, 258, 260, 267, 268, 272, 280, 301, 323, 326, 330, 331, 361, 376, 389, 401, 420, 421, 423, 424, 426, 440, 445, 451, 452, 454, 456, 458, 463, 468, 470, 477, 478, 482, 490, 511, 533, 536, 540, 541, 571, 586, 599, 611, 630, 635, 650, 666, 667, 676, 679, 695, 704, 706, 708, 717, 722, 724, 729, 743, 754, 757, 758, 773, 774, 779, 782, 785, 799, 804, 812, 824, 834, 835, 836, 841, 843, 845, 847, 854, 856, 860, 864, 869, 899, 922, 923, 925, 932, 935, 936, 950, 957, 981, 982, 994, 999, 1002, 1007, 1008, 1011, 1024, 1028, 1042, 1044, 1045, 1047, 1061, 1066, 1072, 1073, 1075, 1077, 1079, 1084, 1089, 1091, 1098, 1099, 1103, 1111, 1132, 1154, 1157, 1161, 1162, 1192, 1207, 1220, 1232, 1251, 1252, 1254, 1255, 1257, 1271, 1276, 1282, 1283, 1285, 1287, 1289, 1294, 1299, 1301, 1308, 1309, 1313, 1321, 1342, 1364, 1367, 1371, 1372, 1402, 1417, 1430, 1442, 1461, 1466, 1481, 1497, 1498, 1507, 1510, 1526, 1535, 1537, 1539, 1548, 1553, 1555, 1560, 1574, 1585, 1588, 1589, 1604, 1605, 1610, 1613, 1616, 1630, 1635, 1643, 1655, 1665, 1666, 1667, 1672, 1674, 1676, 1678, 1685, 1687, 1691, 1695, 1700, 1730, 1753, 1754, 1756, 1763, 1766, 1767, 1781, 1788, 1812, 1813, 1825, 1830, 1833, 1838, 1839, 1842, 1855, 1859, 1873, 1875, 1876, 1878, 1892, 1897, 1903, 1904, 1906, 1908, 1910, 1915, 1920, 1922, 1929, 1930, 1934, 1942, 1963, 1985, 1988, 1992, 1993, 2023, 2038, 2051, 2063, 2082, 2083, 2085, 2086, 2088, 2102, 2107, 2113, 2114, 2116, 2118, 2120, 2125, 2130, 2132, 2139, 2140, 2144, 2152, 2173, 2195, 2198, 2202, 2203, 2233, 2248, 2261, 2273, 2292, 2297, 2312, 2328, 2329, 2338, 2341, 2357, 2366, 2368, 2370, 2379, 2384, 2386, 2391, 2405, 2416, 2419, 2420, 2435, 2436, 2441, 2444, 2447, 2461, 2466, 2474, 2486, 2496, 2497, 2498, 2503, 2505, 2507, 2509, 2516, 2518, 2522, 2526, 2531, 2561, 2584, 2585, 2587, 2594, 2597, 2598, 2612, 2619, 2643, 2644, 2656, 2661, 2664, 2669, 2670, 2673, 2686, 2690, 2704, 2706, 2707, 2709, 2723, 2728, 2734, 2735, 2737, 2739, 2741, 2746, 2751, 2753, 2760, 2761, 2765, 2773, 2794, 2816, 2819, 2823, 2824, 2854, 2869, 2882, 2894, 2913, 2914, 2916, 2917, 2919, 2933, 2938, 2944, 2945, 2947, 2949, 2951, 2956, 2961, 2963, 2970, 2971, 2975, 2983, 3004, 3026, 3029, 3033, 3034, 3064, 3079, 3092, 3104, 3123, 3128, 3143, 3159, 3160, 3169, 3172, 3188, 3197, 3199, 3201, 3210, 3215, 3217, 3222, 3236, 3247, 3250, 3251, 3266, 3267, 3272, 3275, 3278, 3292, 3297, 3305, 3317, 3327, 3328, 3329, 3334, 3336, 3338, 3340, 3347, 3349, 3353, 3357, 3362, 3392, 3415, 3416, 3418, 3425, 3428, 3429, 3443, 3450, 3474, 3475, 3487, 3492, 3495, 3500, 3501, 3504, 3517, 3521, 3535, 3537, 3538, 3540, 3554, 3559, 3565, 3566, 3568, 3570, 3572, 3577, 3582, 3584, 3591, 3592, 3596, 3604, 3625, 3647, 3650, 3654, 3655, 3685, 3700, 3713, 3725, 3744, 3745, 3747, 3748, 3750, 3764, 3769, 3775, 3776, 3778, 3780, 3782, 3787, 3792, 3794, 3801, 3802, 3806, 3814, 3835, 3857, 3860, 3864, 3865, 3895, 3910, 3923, 3935, 3954, 3959, 3974, 3990, 3991, 4000, 4003, 4019, 4028, 4030, 4032, 4041, 4046, 4048, 4053, 4067, 4078, 4081, 4082, 4097, 4098, 4103, 4106, 4109, 4123, 4128, 4136, 4148, 4158, 4159, 4160, 4165, 4167, 4169, 4171, 4178, 4180, 4184, 4188, 4193, 4223, 4246, 4247, 4249, 4256, 4259, 4260, 4274, 4281, 4305, 4306, 4318, 4323, 4326, 4331, 4332, 4335, 4348, 4352, 4366, 4368, 4369, 4371, 4385, 4390, 4396, 4397, 4399, 4401, 4403, 4408, 4413, 4415, 4422, 4423, 4427, 4435, 4456, 4478, 4481, 4485, 4486, 4516, 4531, 4544, 4556, 4575, 4576, 4578, 4579, 4581, 4595, 4600, 4606, 4607, 4609, 4611, 4613, 4618, 4623, 4625, 4632, 4633, 4637, 4645, 4666, 4688, 4691, 4695, 4696, 4726, 4741, 4754, 4766, 4785, 4790, 4805, 4821, 4822, 4831, 4834, 4850, 4859, 4861, 4863, 4872, 4877, 4879, 4884, 4898, 4909, 4912, 4913, 4928, 4929, 4934, 4937, 4940, 4954, 4959, 4967, 4979, 4989, 4990, 4991, 4996, 4998, 5000, 5002, 5009, 5011, 5015, 5019, 5024, 5054, 5077, 5078, 5080, 5087, 5090, 5091, 5105, 5112, 5136, 5137, 5149, 5154, 5157, 5162, 5163, 5166, 5179, 5183, 5197, 5199, 5200, 5202, 5216, 5221, 5227, 5228, 5230, 5232, 5234, 5239, 5244, 5246, 5253, 5254, 5258, 5266, 5287, 5309, 5312, 5316, 5317, 5347, 5362, 5375, 5387, 5406, 5407, 5409, 5410, 5412, 5426, 5431, 5437, 5438, 5440, 5442, 5444, 5449, 5454, 5456, 5463, 5464, 5468, 5476, 5497, 5519, 5522, 5526, 5527, 5557, 5572, 5585, 5597, 5616, 5621, 5636, 5652, 5653, 5662, 5665, 5681, 5690, 5692, 5694, 5703, 5708, 5710, 5715, 5729, 5740, 5743, 5744, 5759, 5760, 5765, 5768, 5771, 5785, 5790, 5798, 5810, 5820, 5821, 5822, 5827, 5829, 5831, 5833, 5840, 5842, 5846, 5850, 5855, 5885, 5908, 5909, 5911, 5918, 5921, 5922, 5936, 5943, 5967, 5968, 5980, 5985, 5988, 5993, 5994, 5997, 6010, 6014, 6028, 6030, 6031, 6033, 6047, 6052, 6058, 6059, 6061, 6063, 6065, 6070, 6075, 6077, 6084, 6085, 6089, 6097, 6118, 6140, 6143, 6147, 6148, 6178, 6193, 6206, 6218, 6237, 6238, 6240, 6241, 6243, 6257, 6262, 6268, 6269, 6271, 6273, 6275, 6280, 6285, 6287, 6294, 6295, 6299, 6307, 6328, 6350, 6353, 6357, 6358, 6388, 6403, 6416, 6428, 6447, 6452, 6467, 6483, 6484, 6493, 6496, 6512, 6521, 6523, 6525, 6534, 6539, 6541, 6546, 6560, 6571, 6574, 6575, 6590, 6591, 6596, 6599, 6602, 6616, 6621, 6629, 6641, 6651, 6652, 6653, 6658, 6660, 6662, 6664, 6671, 6673, 6677, 6681, 6686, 6716, 6739, 6740, 6742, 6749, 6752, 6753, 6767, 6774, 6798, 6799, 6811, 6816, 6819, 6824, 6825, 6828, 6841, 6845, 6859, 6861, 6862, 6864, 6878, 6883, 6889, 6890, 6892, 6894, 6896, 6901, 6906, 6908, 6915, 6916, 6920, 6928, 6949, 6971, 6974, 6978, 6979, 7009, 7024, 7037, 7049, 7068, 7069, 7071, 7072, 7074, 7088, 7093, 7099, 7100, 7102, 7104, 7106, 7111, 7116, 7118, 7125, 7126, 7130, 7138, 7159, 7181, 7184, 7188, 7189, 7219, 7234, 7247, 7259, 7278, 7283, 7298, 7314, 7315, 7324, 7327, 7343, 7352, 7354, 7356, 7365, 7370, 7372, 7377, 7391, 7402, 7405, 7406, 7421, 7422, 7427, 7430, 7433, 7447, 7452, 7460, 7472, 7482, 7483, 7484, 7489, 7491, 7493, 7495, 7502, 7504, 7508, 7512, 7517, 7547, 7570, 7571, 7573, 7580, 7583, 7584, 7598, 7605, 7629, 7630, 7642, 7647, 7650, 7655, 7656, 7659, 7672, 7676, 7690, 7692, 7693, 7695, 7709, 7714, 7720, 7721, 7723, 7725, 7727, 7732, 7737, 7739, 7746, 7747, 7751, 7759, 7780, 7802, 7805, 7809, 7810, 7840, 7855, 7868, 7880, 7899, 7900, 7902, 7903, 7905, 7919, 7924, 7930, 7931, 7933, 7935, 7937, 7942, 7947, 7949, 7956, 7957, 7961, 7969, 7990, 8012, 8015, 8019, 8020, 8050, 8065, 8078, 8090, 8109, 8114, 8129, 8145, 8146, 8155, 8158, 8174, 8183, 8185, 8187, 8196, 8201, 8203, 8208, 8222, 8233, 8236, 8237, 8252, 8253, 8258, 8261, 8264, 8278, 8283, 8291, 8303, 8313, 8314, 8315, 8320, 8322, 8324, 8326, 8333, 8335, 8339, 8343, 8348, 8378, 8401, 8402, 8404, 8411, 8414, 8415, 8429, 8436, 8460, 8461, 8473, 8478, 8481, 8486, 8487, 8490, 8503, 8507, 8521, 8523, 8524, 8526, 8540, 8545, 8551, 8552, 8554, 8556, 8558, 8563, 8568, 8570, 8577, 8578, 8582, 8590, 8611, 8633, 8636, 8640, 8641, 8671, 8686, 8699, 8711, 8730, 8731, 8733, 8734, 8736, 8750, 8755, 8761, 8762, 8764, 8766, 8768, 8773, 8778, 8780, 8787, 8788, 8792, 8800, 8821, 8843, 8846, 8850, 8851, 8881, 8896, 8909, 8921, 8940, 8945, 8960, 8976, 8977, 8986, 8989, 9005, 9014, 9016, 9018, 9027, 9032, 9034, 9039, 9053, 9064, 9067, 9068, 9083, 9084, 9089, 9092, 9095, 9109, 9114, 9122, 9134, 9144, 9145, 9146, 9151, 9153, 9155, 9157, 9164, 9166, 9170, 9174, 9179, 9209, 9232, 9233, 9235, 9242, 9245, 9246, 9260, 9267, 9291, 9292, 9304, 9309, 9312, 9317, 9318, 9321, 9334, 9338]\n",
      "Token 'and' is not found in any document.\n",
      "Token 'java' is not found in any document.\n",
      "Token 'mastered' is not found in any document.\n",
      "Token 'in' is not found in any document.\n",
      "Token 'pointers' is found in documents: [53, 85, 86, 118, 151, 209, 263, 295, 296, 328, 361, 419, 473, 505, 506, 538, 571, 629, 686, 687, 731, 738, 739, 787, 791, 799, 822, 882, 883, 942, 947, 948, 1016, 1017, 1094, 1126, 1127, 1159, 1192, 1250, 1304, 1336, 1337, 1369, 1402, 1460, 1517, 1518, 1562, 1569, 1570, 1618, 1622, 1630, 1653, 1713, 1714, 1773, 1778, 1779, 1847, 1848, 1925, 1957, 1958, 1990, 2023, 2081, 2135, 2167, 2168, 2200, 2233, 2291, 2348, 2349, 2393, 2400, 2401, 2449, 2453, 2461, 2484, 2544, 2545, 2604, 2609, 2610, 2678, 2679, 2756, 2788, 2789, 2821, 2854, 2912, 2966, 2998, 2999, 3031, 3064, 3122, 3179, 3180, 3224, 3231, 3232, 3280, 3284, 3292, 3315, 3375, 3376, 3435, 3440, 3441, 3509, 3510, 3587, 3619, 3620, 3652, 3685, 3743, 3797, 3829, 3830, 3862, 3895, 3953, 4010, 4011, 4055, 4062, 4063, 4111, 4115, 4123, 4146, 4206, 4207, 4266, 4271, 4272, 4340, 4341, 4418, 4450, 4451, 4483, 4516, 4574, 4628, 4660, 4661, 4693, 4726, 4784, 4841, 4842, 4886, 4893, 4894, 4942, 4946, 4954, 4977, 5037, 5038, 5097, 5102, 5103, 5171, 5172, 5249, 5281, 5282, 5314, 5347, 5405, 5459, 5491, 5492, 5524, 5557, 5615, 5672, 5673, 5717, 5724, 5725, 5773, 5777, 5785, 5808, 5868, 5869, 5928, 5933, 5934, 6002, 6003, 6080, 6112, 6113, 6145, 6178, 6236, 6290, 6322, 6323, 6355, 6388, 6446, 6503, 6504, 6548, 6555, 6556, 6604, 6608, 6616, 6639, 6699, 6700, 6759, 6764, 6765, 6833, 6834, 6911, 6943, 6944, 6976, 7009, 7067, 7121, 7153, 7154, 7186, 7219, 7277, 7334, 7335, 7379, 7386, 7387, 7435, 7439, 7447, 7470, 7530, 7531, 7590, 7595, 7596, 7664, 7665, 7742, 7774, 7775, 7807, 7840, 7898, 7952, 7984, 7985, 8017, 8050, 8108, 8165, 8166, 8210, 8217, 8218, 8266, 8270, 8278, 8301, 8361, 8362, 8421, 8426, 8427, 8495, 8496, 8573, 8605, 8606, 8638, 8671, 8729, 8783, 8815, 8816, 8848, 8881, 8939, 8996, 8997, 9041, 9048, 9049, 9097, 9101, 9109, 9132, 9192, 9193, 9252, 9257, 9258, 9326, 9327]\n"
     ]
    }
   ],
   "source": [
    "input_sentence = input(\"Enter the search sentence: \")\n",
    "tokens_to_search = word_tokenize(input_sentence.lower())  # Tokenize the sentence into words\n",
    "\n",
    "# Initialize an empty dictionary to store search results\n",
    "search_results = {}\n",
    "\n",
    "# Directly search for each token in the posting list\n",
    "for token in tokens_to_search:\n",
    "    # Fetch the posting list for the token\n",
    "    if token in posting_list:\n",
    "        search_results[token] = posting_list[token]\n",
    "    else:\n",
    "        search_results[token] = []\n",
    "\n",
    "# Display the search results\n",
    "for token, docs in search_results.items():\n",
    "    if docs:\n",
    "        print(f\"Token '{token}' is found in documents: {docs}\")\n",
    "    else:\n",
    "        print(f\"Token '{token}' is not found in any document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01828afb-e2e8-4d4f-9fd0-09648830f06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
